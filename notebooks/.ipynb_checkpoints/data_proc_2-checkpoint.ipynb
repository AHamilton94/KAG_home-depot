{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Author: Alastair Hamilton\n",
    "# Date: May/June 2018\n",
    "# Title: Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Misc\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint as pp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## NLP\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Pandas error display OFF\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Set path to data\n",
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Processing features\n",
    "proc_feat = ['search_term', 'product_title', 'product_description', 'attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tokenise a pandas Series\n",
    "def tokenise(s, tokeniser, tokenise_fn=False):\n",
    "    if tokenise_fn:\n",
    "        return s.apply(tokeniser.tokenize)\n",
    "    else:\n",
    "        return s.apply(tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stem(s, stemmer):\n",
    "    return s.apply(lambda x: tuple(map(stemmer.stem, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation from a pandas Series\n",
    "def rmv_punc(s):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: not y.is_punct, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def rmv_stop(s, stops):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: y not in stops, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply function on rows of data frame (2 cols max)\n",
    "def func_row(df, func):\n",
    "    return df.apply(lambda row: func(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find number of words in one document (doc1) that are in another document (doc2)\n",
    "def common_words_doc(doc1, doc2):\n",
    "    tot = 0\n",
    "    for w1 in doc1:\n",
    "        for w2 in doc2:\n",
    "            if w2.find(w1) >= 0:\n",
    "                tot += 1\n",
    "                break\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "- Importing attributes.csv.zip...\n",
      "- Importing test.csv.zip...\n",
      "- Importing train.csv.zip...\n",
      "- Importing product_descriptions.csv.zip...\n",
      "- Importing sample_submission.csv.zip...\n"
     ]
    }
   ],
   "source": [
    "print('Importing data...')\n",
    "\n",
    "# # Get all zipped files in data path\n",
    "zips = [f for f in os.listdir(data_path) if re.search(\".zip$\", f)]\n",
    "\n",
    "# # Unzip all files and put into dictionary, keyed by file stem\n",
    "data_dict = {}\n",
    "for zipped in zips:\n",
    "    print('- Importing {}...'.format(zipped))\n",
    "    data_dict[zipped.split('.')[0]] = pd.read_csv(data_path+zipped, compression='zip', encoding='latin1')\n",
    "\n",
    "# # Set dataframe to piece in data dictionary\n",
    "train_df = data_dict['train']\n",
    "prod_desc = data_dict['product_descriptions']\n",
    "attributes = data_dict['attributes']\n",
    "\n",
    "# # Clean up\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "- Handling attributes data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data...\")\n",
    "\n",
    "# # Process attributes data\n",
    "print(\"- Handling attributes data...\")\n",
    "\n",
    "# # # Deal with N/As in attributes data (drop empty records and fill in name and values with empty string)\n",
    "attr = attributes.dropna(how='all')\n",
    "attr[['name','value']] = attr[['name','value']].fillna('')\n",
    "\n",
    "# # # Ensure UID is int\n",
    "attr['product_uid'] = attr['product_uid'].apply(lambda x: int(x))\n",
    "\n",
    "# # # If \"bullet\" in attribute name then asserting name is meaningless - make an empty string\n",
    "attr['name'] = attr['name'].apply(lambda x: '' if \"Bullet\" in x else x)\n",
    "\n",
    "# # # Group name and value in attributes into single column, separated by a tab and ending in newline (for grouping stage next)\n",
    "attr['attributes'] = attr['name'] + '\\t' + attr['value'] + '\\n'\n",
    "\n",
    "# # # Drop name and values, groupby UID and sum grouped values, reset index...\n",
    "# # # ...(ie. all attributes in single cell now, separated by newlines as set up above)\n",
    "attr = attr.drop(['name','value'], axis=1).groupby('product_uid').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Creating master data frame...\n"
     ]
    }
   ],
   "source": [
    "# # Create master data frame\n",
    "print(\"- Creating master data frame...\")\n",
    "\n",
    "# # # Merge all data into one master dataframe by merging descriptions and attributes onto training data on UID...\n",
    "# # # ...Fill any NAs with empty string\n",
    "data = pd.merge(train_df, prod_desc, how='left',\n",
    "                on='product_uid').drop('id', axis=1).merge(attr, on='product_uid', how='left').fillna('')\n",
    "\n",
    "# # # Finally create a master index column, which will be used to reference individual search terms\n",
    "data = data.reset_index(drop=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up\n",
    "del train_df\n",
    "del prod_desc\n",
    "del attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "data has ```74067``` rows.\n",
    "- Whole pipeline takes ~16s/column (pipeline column which isn't as rich as some of the others) [9/5/19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['product_title', 'search_term', 'product_description', 'product_description', 'attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation\n",
    "Using the regexp tokeniser in NLTK with ```r'\\w+'``` was significantly faster and got rid of punctuation, which was intended.\n",
    "\n",
    "- Need to add it to ignore the weird a^ character\n",
    "\n",
    "|Tokeniser|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|Regexp|1.28|\n",
    "|wordpunct|1.57|\n",
    "|wordtokenise|48.5|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_anon = lambda x: tokenise(x, RegexpTokenizer(r'\\w+'), tokenise_fn=True)\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(tokenise_anon, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n",
    "Caching the stop words corpus was ridiculously faster (ie loading ```stopwords.words('english')``` once).\n",
    "\n",
    "|Remover|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|nltk stop corpus filter (no cache)|435|\n",
    "|nltk stop corpus filter (cache)|6.71|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_anon = lambda x: rmv_stop(x, stopwords.words('english'))\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(stopwords_anon, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Worth seeing if this affects the model further down the line. \n",
    "- Choosing snowball as faster and seen it be used for this problem before [9/5/19]\n",
    "\n",
    "|Stemmer|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|Porter|45|\n",
    "|Snowball|37.9|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_anon = lambda x: stem(x, SnowballStemmer('english'))\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(stemming_anon, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>(simpson, strong, tie, 12, gaug, angl)</td>\n",
       "      <td>(angl, bracket)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(not, angl, make, joint, stronger, also, provi...</td>\n",
       "      <td>(versatil, connector, various, 90â, connect, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>(simpson, strong, tie, 12, gaug, angl)</td>\n",
       "      <td>(l, bracket)</td>\n",
       "      <td>2.5</td>\n",
       "      <td>(not, angl, make, joint, stronger, also, provi...</td>\n",
       "      <td>(versatil, connector, various, 90â, connect, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                           product_title      search_term  \\\n",
       "0       100001  (simpson, strong, tie, 12, gaug, angl)  (angl, bracket)   \n",
       "1       100001  (simpson, strong, tie, 12, gaug, angl)     (l, bracket)   \n",
       "\n",
       "   relevance                                product_description  \\\n",
       "0        3.0  (not, angl, make, joint, stronger, also, provi...   \n",
       "1        2.5  (not, angl, make, joint, stronger, also, provi...   \n",
       "\n",
       "                                          attributes  \n",
       "0  (versatil, connector, various, 90â, connect, h...  \n",
       "1  (versatil, connector, various, 90â, connect, h...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feat = pd.DataFrame(data_proc['product_uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Len of query\n",
    "data_feat['q_len'] = data_proc['search_term'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product title\n",
    "data_feat['com_title'] = func_row(data_proc[['search_term', 'product_title']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "data_feat['com_desc'] = func_row(data_proc[['search_term', 'product_description']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned attributes\n",
    "data_feat['com_attr'] = func_row(data_proc[['search_term', 'attributes']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>q_len</th>\n",
       "      <th>com_title</th>\n",
       "      <th>com_desc</th>\n",
       "      <th>com_attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_uid  q_len  com_title  com_desc  com_attr\n",
       "0      100001    2.0        1.0       1.0       1.0\n",
       "1      100001    2.0        1.0       1.0       1.0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write data to file\n",
    "data_feat.to_csv(data_path+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home-depot",
   "language": "python",
   "name": "home-depot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
