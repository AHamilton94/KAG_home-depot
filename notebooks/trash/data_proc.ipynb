{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Author: Alastair Hamilton\n",
    "# Date: May/June 2018\n",
    "# Title: Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Misc\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "\n",
    "## NLP\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas error display OFF\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set path to data\n",
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create spaCy nlp tagger\n",
    "spacy.prefer_gpu()\n",
    "nlp_tag = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Processing features\n",
    "proc_feat = ['search_term', 'product_title', 'product_description', 'attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from column of dataframe\n",
    "def rmv_punc(df, col):\n",
    "    df.loc[:, col] = df.loc[:,col].apply(lambda x: tuple(filter(lambda y: not y.is_punct, x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def rmv_stoppunc(s):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: not (y.is_stop or y.is_punct), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function on rows of data frame (2 cols max)\n",
    "def func_row(df, func):\n",
    "    return df.apply(lambda row: func(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of words in one document (doc1) that are in another document (doc2)\n",
    "def common_words_doc(doc1, doc2):\n",
    "    tot = 0\n",
    "    for w1 in doc1:\n",
    "        for w2 in doc2:\n",
    "            if w2.lemma_.find(w1.lemma_) >= 0:\n",
    "                tot += 1\n",
    "                break\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing data...')\n",
    "\n",
    "# # Get all zipped files in data path\n",
    "zips = [f for f in os.listdir(data_path) if re.search(\".zip$\", f)]\n",
    "\n",
    "# # Unzip all files and put into dictionary, keyed by file stem\n",
    "data_dict = {}\n",
    "for zipped in zips:\n",
    "    print('- Importing {}...'.format(zipped))\n",
    "    data_dict[zipped.split('.')[0]] = pd.read_csv(data_path+zipped, compression='zip', encoding='latin1')\n",
    "\n",
    "# # Set dataframe to piece in data dictionary\n",
    "train_df = data_dict['train']\n",
    "prod_desc = data_dict['product_descriptions']\n",
    "attributes = data_dict['attributes']\n",
    "\n",
    "# # Clean up\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing data...\")\n",
    "\n",
    "# # Process attributes data\n",
    "print(\"- Handling attributes data...\")\n",
    "\n",
    "# # # Deal with N/As in attributes data (drop empty records and fill in name and values with empty string)\n",
    "attr = attributes.dropna(how='all')\n",
    "attr[['name','value']] = attr[['name','value']].fillna('')\n",
    "\n",
    "# # # Ensure UID is int\n",
    "attr['product_uid'] = attr['product_uid'].apply(lambda x: int(x))\n",
    "\n",
    "# # # If \"bullet\" in attribute name then asserting name is meaningless - make an empty string\n",
    "attr['name'] = attr['name'].apply(lambda x: '' if \"Bullet\" in x else x)\n",
    "\n",
    "# # # Group name and value in attributes into single column, separated by a tab and ending in newline (for grouping stage next)\n",
    "attr['attributes'] = attr['name'] + '\\t' + attr['value'] + '\\n'\n",
    "\n",
    "# # # Drop name and values, groupby UID and sum grouped values, reset index...\n",
    "# # # ...(ie. all attributes in single cell now, separated by newlines as set up above)\n",
    "attr = attr.drop(['name','value'], axis=1).groupby('product_uid').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create master data frame\n",
    "print(\"- Creating master data frame...\")\n",
    "\n",
    "# # # Merge all data into one master dataframe by merging descriptions and attributes onto training data on UID...\n",
    "# # # ...Fill any NAs with empty string\n",
    "data = pd.merge(train_df, prod_desc, how='left', on='product_uid').drop('id', axis=1).merge(attr, on='product_uid', how='left').fillna('')\n",
    "\n",
    "# # # Finally create a master index column, which will be used to reference individual search terms\n",
    "data = data.reset_index().drop('index', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up\n",
    "del train_df\n",
    "del prod_desc\n",
    "del attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Len of query\n",
    "data['q_len'] = data['search_term'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product title\n",
    "data['com_title'] = func_row(data[['search_term', 'product_title']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "data['com_desc'] = func_row(data[['search_term', 'product_description']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "data['com_attr'] = func_row(data[['search_term', 'attributes']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up\n",
    "data = data.drop(proc_feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write data to file\n",
    "data.to_csv(data_path+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home-depot",
   "language": "python",
   "name": "home-depot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
