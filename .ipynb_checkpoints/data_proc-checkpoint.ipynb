{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Alastair Hamilton\n",
    "# Date: May/June 2018\n",
    "# Title: Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Misc\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint as pp\n",
    "import time\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "## NLP\n",
    "import spacy\n",
    "\n",
    "## ML\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas error display OFF\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set path to data\n",
    "data_path = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create spaCy nlp tagger\n",
    "nlp_tag = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Processing features\n",
    "proc_feat = ['search_term', 'product_title', 'product_description', 'attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from column of dataframe\n",
    "def rmv_punc(df, col):\n",
    "    df.loc[:, col] = df.loc[:,col].apply(lambda x: tuple(filter(lambda y: not y.is_punct, x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation\n",
    "def rmv_stoppunc(s):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: not (y.is_stop or y.is_punct), x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function on rows of data frame (2 cols max)\n",
    "def func_row(df, func):\n",
    "    return df.apply(lambda row: func(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of words in one document (doc1) that are in another document (doc2)\n",
    "def common_words_doc(doc1, doc2):\n",
    "    tot = 0\n",
    "    for w1 in doc1:\n",
    "        for w2 in doc2:\n",
    "#             if w1.lemma_ == w2.lemma_:\n",
    "#                 tot += 1\n",
    "#                 break\n",
    "            if w2.lemma_.find(w1.lemma_) >= 0:\n",
    "                tot += 1\n",
    "                break\n",
    "    return tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importing data...')\n",
    "\n",
    "# # Get all zipped files in data path\n",
    "zips = [f for f in os.listdir(data_path) if re.search(\".zip$\", f)]\n",
    "\n",
    "# # Unzip all files and put into dictionary, keyed by file stem\n",
    "data_dict = {}\n",
    "for zipped in zips:\n",
    "    print('- Importing {}...'.format(zipped))\n",
    "    data_dict[zipped.split('.')[0]] = pd.read_csv(data_path+zipped, compression='zip', encoding='latin1')\n",
    "\n",
    "# # Set dataframe to piece in data dictionary\n",
    "train_df = data_dict['train']\n",
    "prod_desc = data_dict['product_descriptions']\n",
    "attributes = data_dict['attributes']\n",
    "\n",
    "# # Clean up\n",
    "del data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing data...\")\n",
    "\n",
    "# # Process attributes data\n",
    "print(\"- Handling attributes data...\")\n",
    "\n",
    "# # # Deal with N/As in attributes data (drop empty records and fill in name and values with empty string)\n",
    "attr = attributes.dropna(how='all')\n",
    "attr[['name','value']] = attr[['name','value']].fillna('')\n",
    "\n",
    "# # # Ensure UID is int\n",
    "attr['product_uid'] = attr['product_uid'].apply(lambda x: int(x))\n",
    "\n",
    "# # # If \"bullet\" in attribute name then asserting name is meaningless - make an empty string\n",
    "attr['name'] = attr['name'].apply(lambda x: '' if \"Bullet\" in x else x)\n",
    "\n",
    "# # # Group name and value in attributes into single column, separated by a tab and ending in newline (for grouping stage next)\n",
    "attr['attributes'] = attr['name'] + '\\t' + attr['value'] + '\\n'\n",
    "\n",
    "# # # Drop name and values, groupby UID and sum grouped values, reset index...\n",
    "# # # ...(ie. all attributes in single cell now, separated by newlines as set up above)\n",
    "attr = attr.drop(['name','value'], axis=1).groupby('product_uid').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create master data frame\n",
    "print(\"- Creating master data frame...\")\n",
    "\n",
    "# # # Merge all data into one master dataframe by merging descriptions and attributes onto training data on UID...\n",
    "# # # ...Fill any NAs with empty string\n",
    "data = pd.merge(train_df, prod_desc, how='left', on='product_uid').drop('id', axis=1).merge(attr, on='product_uid', how='left').fillna('')\n",
    "\n",
    "# # # Finally create a master index column, which will be used to reference individual search terms\n",
    "data = data.reset_index().drop('index', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up\n",
    "del train_df\n",
    "del prod_desc\n",
    "del attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLP\n",
    "print(\"- Applying spaCy NLP processor to all string features...\")\n",
    "for feat in proc_feat:\n",
    "    print('-- Applying to {}'.format(feat))\n",
    "    data[feat] = data[feat].apply(nlp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Len of query\n",
    "print('- Creating length of query column...')\n",
    "data['q_len'] = data['search_term'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product title\n",
    "print('- Creating query-title common words column...')\n",
    "data['com_title'] = func_row(data[['search_term', 'product_title']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "print('- Creating query-description common words column...')\n",
    "data['com_desc'] = func_row(data[['search_term', 'product_description']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "print('- Creating query-attributes common words column...')\n",
    "data['com_attr'] = func_row(data[['search_term', 'attributes']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up\n",
    "data = data.drop(proc_feat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write data to file\n",
    "data.to_csv(data_path+'features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:earth]",
   "language": "python",
   "name": "conda-env-earth-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
